"""
Author: Andreas Bott
year: 2024
mail: andreas.bott@eins.tu-darmstadt.de or andi-bott@web.de


This script generates samples for random demands and fixed plant parameters (except for the slack plant's power).
The samples are generated by solving the steady state equations directly. The results are saved in csv files which are
numbered consecutively.

csv file format:
    columns: [Q1, Q2, Q3, Q4, Q0], [T1, T2, T3, T4, T0], [[grid state vector == (T, mf, p, T_end)]]

The script is designed to be run in parallel, where each instance computes a fraction of the samples.
:param n_samples: number of samples generated by all instances
:param n_instances: number of instances running in parallel
:param instant: integer argument to distinguish the different instances expected to be passed as sys.argv[1]
"""


import os
import sys

from lib_dnn_soc.steady_state_modelling.steady_state_solvers import solve_SE, MaximumIterationException
from lib_dnn_soc.steady_state_modelling.state_equations import TemperatureProbagationException
import lib_dnn_soc.utility as util
import warnings
import tensorflow as tf

def __main__(results_file, grid_settings, q_fix, T_fix, n_samples, path, save_file, verbose=False):
    if not os.path.exists(path):
        os.makedirs(path)

    #%% Section 1: Setup
    Results = util.ResFile(name=results_file, path='./results/', print=verbose)
    Results.log('Run showcase for the APC problem')

    SE, d_prior_dist, T_prior_dist, cycles, grid = util.setup_training(grid_identifier, grid_settings)

    # draw random input samples:
    d_samples = d_prior_dist.sample(n_samples)
    T_samples = T_prior_dist.sample(n_samples)

    # solve each sample:
    for i, (d, T) in enumerate(zip(d_samples, T_samples)):
        if verbose:
            print(f'calculating sample {i}/{n_samples}')
        # set demands
        SE.load_save_state()
        d = tf.tensor_scatter_nd_update(d, [[3]], [q_fix])
        T = tf.tensor_scatter_nd_update(T, [[3], [4]], T_fix)
        while tf.reduce_sum(d) < 0:
            d = d_prior_dist.sample(1)
            d = tf.tensor_scatter_nd_update(d, [[3]], [q_fix])
        try:
            solve_SE(d, T, SE, cycles)
        except MaximumIterationException:
            continue
        # save results
        Q_vals = SE.get_demand_from_grid(heating=True)
        state = tf.concat([SE.T, SE.mf, SE.p, SE.T_end], axis=0)
        tf.print(Q_vals, T, tf.transpose(state), summarize=-1,
                 output_stream='file://' + f'{save_file(i)}')



if __name__ == '__main__':
    grid_identifier = 'ladder5'                 # selector for different heating grids
    results_file = 'results_smaller_nodem.out'  # file to save the results
    from Settings import grid_settings          # load the grid settings from the configuration file
    q_fix = -275                                # fixed power value for the non-slack power plant
    T_fix = [85., 95.]                          # fixed temperature values PP4, PP0
    n_samples = 10_000                          # number of samples to generate
    n_per_file = 2_000                          # number of samples per file
    path = f'./data/{grid_identifier}/'         # path, where the CSV files are stored
    n_instances = 1                             # number of simultaneous runs, each computing a fraction of the samples

    # in order to avoid overwriting existing files, n_samples / n_per_file * n_runs should be an integer
    assert n_samples % (n_per_file * n_instances) == 0, (f'n_samples / (n_per_file * n_runs) should be an integer; '
                                                         f'n_samples: {n_samples}, n_per_file: {n_per_file}, '
                                                         f'n_runs: {n_instances}')

    # number of samples computed by this instance:
    n_samples = int(n_samples / n_instances)
    instant = 0 if len(sys.argv) < 2 else int(sys.argv[1])

    # choose save file dynamically to fill up files subsequently. Starting file depends on the instance number.
    def save_file(idx):
        zerofile = instant * (n_samples // n_per_file)
        folder = f'DNN_model/data/{grid_identifier}_{grid_settings["d_prior_type"]}/'
        file = f'{grid_identifier}_{idx // n_per_file + zerofile}.csv'
        return folder + file

__main__(results_file, grid_settings, q_fix, T_fix, n_samples, path, save_file, verbose=True)